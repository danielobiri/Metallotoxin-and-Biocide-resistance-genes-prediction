{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1634 prefered Positive sequence length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3ba5e0e135c0>:50: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(file_name,\"rU\") as handle:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Combinining positive dataset from Uniprot and CARD databases after CD-HIT\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from Bio import SeqIO\n",
    "from nltk import bigrams\n",
    "import gensim, logging\n",
    "import numpy as np\n",
    "from nltk import trigrams\n",
    "#from bert import tokenization\n",
    "from keras import backend as K\n",
    "#pip install --upgrade tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pydot as pyd\n",
    "#from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Embedding, LSTM, GRU\n",
    "from keras.initializers import Constant\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, RepeatVector,Dropout, Conv1D, MaxPooling1D, UpSampling1D,GlobalMaxPool1D\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import random\n",
    "random.seed(12345)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']='0'\n",
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "#tf.set_random_seed(12345)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chars_per_line(file_name):\n",
    "    with open(file_name,\"rU\") as handle:\n",
    "            #outputfile=open(\"The positive cases\",\"w\")\n",
    "            #nega_bact=open(\"Negative set\",'w')\n",
    "            Posit=SeqIO.parse(handle, \"fasta\")\n",
    "            Posit_lis=[record for record in Posit                          if len(record.seq) >= 30]\n",
    "            \n",
    "            \n",
    "  \n",
    "            print(\"Found %i prefered Positive sequence length\" %len(Posit_lis))\n",
    "            SeqIO.write(Posit_lis[0:455],\"Metallic_pre_testing_training_set.fasta\",\"fasta\")\n",
    "            \n",
    "            #print(curated_seq_positives[0:5])\n",
    "#chars_per_line(r\"C:\\Users\\AMD\\Desktop\\Specific\\metals\\BacMet2_EXP_database_1\")\n",
    "chars_per_line(r\"C:\\Users\\AMD\\Desktop\\Pre_metall\\50%_Pre_BacMet.1\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 753 prefered Positive sequence length\n",
      "The minimum Corpus sequence length is: 30\n",
      "The maximum Corpus sequence length is: 1548\n",
      "Found 1506 prefered sequence length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-585ae8bd2485>:2: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(file_name,\"rU\") as handle:\n",
      "<ipython-input-3-585ae8bd2485>:16: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(file_name,\"rU\") as handle:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfilenames =r\"Positive_Dataset_Corpus.fasta\",r\"Negative_Dataset_Corpus.fasta\",r\"Negative_100% perfect.tar\"\\nwith open(\\'Corpus_dataset\\', \\'w\\') as outfile:\\n    for fname in filenames:\\n        with open(fname) as infile:\\n            for line in infile:\\n                outfile.write(line)\\n\\n#Creating Corpus dataset from combined dataset\\nCorpus_set=SeqIO.parse(\\'Corpus_dataset\\',\\'fasta\\')\\nCorpus_set=[record for record in Corpus_set]\\n#print(\"Found %i prefered sequence length\" %len(Training_set_seq))\\nprint(\"The minimum Corpus sequence length is:\",min(len(seq) for seq in Corpus_set))\\nprint(\"The maximum Corpus sequence length is:\",max(len(seq) for seq in Corpus_set))\\nprint(\"Found %i prefered sequence length\" %len(Corpus_set))\\nSeqIO.write(Corpus_set,\"Corpus_dataset.fasta\",\"fasta\")\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def chars_per_line(file_name):\n",
    "    with open(file_name,\"rU\") as handle:\n",
    "            #outputfile=open(\"The positive cases\",\"w\")\n",
    "            #nega_bact=open(\"Negative set\",'w')\n",
    "            Negat=SeqIO.parse(handle, \"fasta\")\n",
    "            Negat_lis=[record for record in Negat if len(record.seq) >= 30]\n",
    "            random.shuffle(Negat_lis)\n",
    "           # print(curated_seq_positives)\n",
    "            #print(\"Found %i prefered negative sequence length\" %len(Negat_lis))\n",
    "            SeqIO.write(Negat_lis,\"Negative_Dataset_Corpus.fasta\",\"fasta\")\n",
    "            SeqIO.write(Negat_lis[0:753],\"Negative_training_set.fasta\",\"fasta\")\n",
    "            #print(curated_seq_positives[0:5])\n",
    "chars_per_line(r\"C:\\Users\\AMD\\Desktop\\New_Patric\\Protein\\Len(450)_New_Susceptible_Negative\")\n",
    "\n",
    "\n",
    "def chars_per_line(file_name):\n",
    "    with open(file_name,\"rU\") as handle:\n",
    "            #outputfile=open(\"The positive cases\",\"w\")\n",
    "            #nega_bact=open(\"Negative set\",'w')\n",
    "            Posit=SeqIO.parse(handle, \"fasta\")\n",
    "            Posit_lis=[record for record in Posit                          if len(record.seq) >= 30]\n",
    "            random.shuffle(Posit_lis)\n",
    "            #print(curated_seq_positives)i\n",
    "            SeqIO.write(Posit_lis,\"Positive_Dataset_Corpus.fasta\",\"fasta\")\n",
    "            print(\"Found %i prefered Positive sequence length\" %len(Posit_lis))\n",
    "            SeqIO.write(Posit_lis[0:753],\"Positive_training_set.fasta\",\"fasta\")\n",
    "            \n",
    "            #print(curated_seq_positives[0:5])\n",
    "chars_per_line(r\"C:\\Users\\AMD\\Desktop\\Specific\\metals\\BacMet2_EXP_database_1\")\n",
    "#chars_per_line(r\"C:\\Users\\AMD\\Desktop\\Pre_metall\\50%_Pre_BacMet.1\")\n",
    "\n",
    "\n",
    "#Combining training dataset (Positive and negative sequences)\n",
    "filenames =r\"Positive_training_set.fasta\",r\"Negative_training_set.fasta\"\n",
    "with open('Training_dataset', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "\n",
    "#Creating training dataset from combined dataset\n",
    "Training_set=SeqIO.parse('Training_dataset','fasta')\n",
    "Training_set=[record for record in Training_set]\n",
    "#print(\"Found %i prefered sequence length\" %len(Training_set_seq))\n",
    "print(\"The minimum Corpus sequence length is:\",min(len(seq) for seq in Training_set))\n",
    "print(\"The maximum Corpus sequence length is:\",max(len(seq) for seq in Training_set))\n",
    "print(\"Found %i prefered sequence length\" %len(Training_set))\n",
    "SeqIO.write(Training_set,\"Metal_Training_Old.fasta\",\"fasta\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Combinining positive & negative for corpus\n",
    "\"\"\"\n",
    "filenames =r\"Positive_Dataset_Corpus.fasta\",r\"Negative_Dataset_Corpus.fasta\",r\"Negative_100% perfect.tar\"\n",
    "with open('Corpus_dataset', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "\n",
    "#Creating Corpus dataset from combined dataset\n",
    "Corpus_set=SeqIO.parse('Corpus_dataset','fasta')\n",
    "Corpus_set=[record for record in Corpus_set]\n",
    "#print(\"Found %i prefered sequence length\" %len(Training_set_seq))\n",
    "print(\"The minimum Corpus sequence length is:\",min(len(seq) for seq in Corpus_set))\n",
    "print(\"The maximum Corpus sequence length is:\",max(len(seq) for seq in Corpus_set))\n",
    "print(\"Found %i prefered sequence length\" %len(Corpus_set))\n",
    "SeqIO.write(Corpus_set,\"Corpus_dataset.fasta\",\"fasta\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-25bec84b0699>:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  print(new_model['LLL'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17453201 -0.1177038  -0.0813143   0.09993038 -0.00133113 -0.15589999\n",
      "  0.10921021 -0.06774119 -0.04655069  0.11801386 -0.0317147  -0.10682648\n",
      "  0.0277033  -0.01161818  0.14001714 -0.3484801   0.00174799  0.14274912\n",
      "  0.11430012  0.2791268   0.303019   -0.05672718 -0.05482263 -0.11991909\n",
      "  0.03766499 -0.03005309 -0.07253376 -0.1322892  -0.05994971  0.08452512\n",
      " -0.06391209 -0.04211859 -0.02986872  0.08940613 -0.3776051   0.07542305\n",
      " -0.00058539  0.00775594  0.27680802 -0.00459834  0.09234035 -0.07007435\n",
      "  0.10985214 -0.12543795  0.13177417 -0.0330069  -0.03330072 -0.04227055\n",
      " -0.08550782 -0.11064211 -0.11114848 -0.01493292  0.11987389 -0.08920457\n",
      "  0.12182682 -0.3234483   0.04045525  0.3173963   0.02877498  0.11675481\n",
      "  0.23227482 -0.02686332 -0.07692467  0.02282335  0.23936945 -0.18473682\n",
      " -0.17016123  0.10775854  0.23902111  0.09823259  0.18978792 -0.15239365\n",
      "  0.14426284  0.12921546 -0.15804538 -0.04675323  0.21542388  0.17162175\n",
      " -0.18767598  0.11513729  0.07930064  0.05190667  0.06811077  0.10480251\n",
      "  0.1855229  -0.28953695  0.00087448 -0.05018714 -0.01643264  0.11014985\n",
      "  0.03364882  0.05405466  0.14364783  0.06240037 -0.03811732 -0.16365528\n",
      "  0.15009585  0.15590614  0.03253964 -0.24629235  0.04787439 -0.1252095\n",
      "  0.02104317 -0.19732939 -0.13542774 -0.20739071 -0.0269584  -0.0598907\n",
      "  0.00584113  0.11027178  0.05494517 -0.1448708   0.07754885  0.11600933\n",
      " -0.06729461  0.14483869 -0.13991903  0.15940028  0.0014863  -0.12921706\n",
      "  0.08944257  0.15744619 -0.06366352  0.07529172 -0.01867106 -0.15808304\n",
      " -0.22833578 -0.00683506 -0.10980966  0.13850634  0.08659569  0.0488696\n",
      " -0.06047399  0.04651268 -0.02100307 -0.17473269  0.16732538 -0.04816576\n",
      " -0.13131176 -0.02580246 -0.12876801  0.15345432 -0.08618946 -0.04410896\n",
      " -0.03563148 -0.06773451 -0.08735867  0.02861129 -0.02499183 -0.01613409\n",
      "  0.02022334 -0.14839381  0.09026042  0.03537494 -0.04974575  0.05325489\n",
      "  0.04281871  0.04085671  0.4089325  -0.08063536 -0.02787902 -0.14457537\n",
      "  0.02402134  0.00919164  0.26722458 -0.03985378  0.03881026 -0.1851719\n",
      "  0.12096117 -0.08670457 -0.02395731 -0.02890434 -0.11753524  0.22091226\n",
      " -0.18433231 -0.0604155   0.12214972  0.10435086  0.01543379 -0.15486516\n",
      " -0.0231517  -0.00981799  0.34917563 -0.12281219 -0.01610302 -0.11286537\n",
      " -0.11328413 -0.04271224 -0.08748402 -0.0851331   0.12992932 -0.06270988\n",
      " -0.11727375 -0.13245909 -0.0036536  -0.0498911   0.0473685  -0.05832559\n",
      "  0.0630738  -0.06984564]\n",
      "[('ELH', 0.5521868467330933), ('LEI', 0.46579211950302124), ('LDL', 0.459644079208374), ('LDV', 0.4322216808795929), ('IDI', 0.4101165235042572), ('IVD', 0.38946306705474854), ('REF', 0.37988877296447754), ('MRN', 0.36811667680740356), ('GMG', 0.3655203580856323), ('WAH', 0.36387795209884644)]\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load the model\"\"\"\n",
    "\n",
    "\n",
    "new_model = gensim.models.Word2Vec.load(r\"C:\\Users\\AMD\\Desktop\\testing\\word2vec_model\")\n",
    "print(new_model['LLL'])\n",
    "print(new_model.wv.most_similar('LDI'))\n",
    "\n",
    "#for i, word in enumerate(new_model.wv.vocab):\n",
    "#    if i == 10:\n",
    "#        break\n",
    "#    print(word)\n",
    "\n",
    "len(new_model.wv.vocab)\n",
    "\n",
    "\n",
    "# In[272]:\n",
    "\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts= []\n",
    "for index, record in enumerate(SeqIO.parse(r\"Metal_Training_Old.fasta\",'fasta')):\n",
    "    tri_tokens = trigrams(record.seq)\n",
    "    temp_str = \"\"\n",
    "    for item in ((tri_tokens)):\n",
    "        #print(item),\n",
    "        temp_str = temp_str + \" \" +item[0] + item[1] + item[2]\n",
    "    #print (temp_str)\n",
    "    texts.append(temp_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9101\n",
      "Found 8122 unique tokens.\n",
      "1506\n",
      "<class 'dict'>\n",
      "Shape of data tensor: (1506, 450)\n",
      "Preparing embedding matrix\n",
      "(8123, 200)\n",
      "[ 2.61605710e-01  2.09528282e-01  8.81497338e-02  1.53352292e-02\n",
      "  8.54821280e-02 -7.79715851e-02  1.22374810e-01  6.31054714e-02\n",
      " -2.60514617e-01 -3.64391021e-02  5.38800061e-02 -1.44904256e-01\n",
      "  5.97710647e-02  7.17352256e-02  1.43319994e-01 -6.66358992e-02\n",
      "  6.82184920e-02 -9.32411849e-02 -2.88900547e-02  1.40857041e-01\n",
      " -3.46783474e-02 -1.55381640e-04 -1.56449363e-01 -1.04431830e-01\n",
      " -1.82619765e-02 -9.16455407e-03 -8.64287913e-02 -2.37787411e-01\n",
      " -7.98403397e-02 -1.23934709e-01 -1.31953776e-01 -4.52549011e-02\n",
      "  1.63900897e-01 -4.74914610e-02 -1.31246701e-01  2.69421607e-01\n",
      " -5.25653735e-02  1.97669074e-01  1.00422338e-01 -7.14662522e-02\n",
      " -9.77328792e-02  1.74318207e-03  1.09561160e-01 -1.87850539e-02\n",
      "  2.57196158e-01 -1.50765702e-01 -1.16017640e-01  3.38845253e-02\n",
      "  2.77004331e-01 -4.82951738e-02 -7.25311562e-02 -1.15469053e-01\n",
      " -4.47242185e-02 -5.00392616e-02  1.65924922e-01 -6.80468902e-02\n",
      " -2.65415721e-02  4.59426455e-02  6.52960241e-02 -1.24257214e-01\n",
      "  1.36087492e-01  1.28680289e-01 -1.80456769e-02 -8.82583335e-02\n",
      "  1.45844787e-01 -3.12923416e-02 -1.17361501e-01  3.12892795e-01\n",
      " -1.11615704e-02  1.01597132e-02 -1.48022594e-02 -1.27541423e-01\n",
      " -8.80379677e-02 -2.14175787e-02 -1.48377061e-01  8.38374496e-02\n",
      " -5.81537820e-02  1.27862915e-01 -1.48298904e-01  1.57171294e-01\n",
      " -1.86746120e-02 -2.45969091e-02  1.11593790e-01 -6.12133443e-02\n",
      "  5.86780757e-02 -1.25427023e-01  7.99994767e-02 -2.31143506e-03\n",
      " -8.83489028e-02 -1.34337293e-02 -1.41048893e-01 -3.39389071e-02\n",
      "  2.05484211e-01 -4.47325669e-02  2.32349783e-01 -3.04430239e-02\n",
      "  5.45472801e-02 -3.68480459e-02 -1.28552243e-01 -1.46258650e-02\n",
      " -7.02551231e-02 -2.08382532e-02  9.93171781e-02  8.10671449e-02\n",
      "  1.54087007e-01 -1.82151973e-01 -2.96057183e-02 -1.16026640e-01\n",
      " -4.94378880e-02 -1.03395648e-01  7.22337216e-02 -1.08194493e-01\n",
      "  7.14833513e-02  1.31461844e-01 -1.75672397e-01 -1.18031343e-02\n",
      "  1.05322696e-01  1.54134154e-03 -1.23723187e-02 -1.09147899e-01\n",
      "  7.70675316e-02  7.05273226e-02  1.77242354e-01  2.21323118e-01\n",
      " -8.66951644e-02 -5.33446483e-03 -3.71042974e-02  2.62635015e-02\n",
      " -8.47407877e-02 -3.81991006e-02 -1.98207229e-01 -3.24648544e-02\n",
      " -1.77033581e-02 -6.93175271e-02  2.50037074e-01 -1.18530855e-01\n",
      " -1.30377039e-02  1.38880491e-01  2.43611977e-01 -2.14724597e-02\n",
      " -1.06435329e-01  5.44071682e-02  6.98405728e-02 -8.71428624e-02\n",
      " -7.14604929e-02  4.98823971e-02  2.67447028e-02 -3.25666252e-03\n",
      "  2.27752745e-01 -1.40821293e-01 -1.76902354e-01 -1.67028189e-01\n",
      "  1.13200061e-01  1.53943971e-01 -1.70898646e-01  2.15892628e-01\n",
      "  5.50220534e-02 -1.64226755e-01 -9.11491066e-02  4.88550067e-02\n",
      "  7.49903321e-02 -2.85468157e-02  8.02231282e-02  7.65135661e-02\n",
      "  6.21929858e-03  2.73323715e-01 -1.14498638e-01 -1.61755029e-02\n",
      "  9.74280238e-02  5.63935302e-02 -2.68329382e-02 -1.07340422e-02\n",
      "  7.80655518e-02  8.28822777e-02  1.03932120e-01 -8.21387246e-02\n",
      "  4.32398655e-02  2.79192090e-01  7.88201243e-02  6.05223179e-02\n",
      "  8.91930237e-02  1.24552079e-01 -1.25638276e-01 -1.31414114e-02\n",
      " -7.44127110e-02 -8.18175822e-03 -3.27059403e-02 -8.91063735e-02\n",
      " -3.31730791e-03 -2.29502976e-01 -1.15937795e-02  8.91798064e-02\n",
      " -1.71183780e-01  1.32300466e-01 -7.38236755e-02 -2.42526736e-02\n",
      "  1.08393215e-01 -6.84785992e-02  1.10347405e-01  9.13171247e-02]\n",
      "8123\n",
      "[('ELH', 0.5521868467330933), ('LEI', 0.46579211950302124), ('LDL', 0.459644079208374), ('LDV', 0.4322216808795929), ('IDI', 0.4101165235042572), ('IVD', 0.38946306705474854), ('REF', 0.37988877296447754), ('MRN', 0.36811667680740356), ('GMG', 0.3655203580856323), ('WAH', 0.36387795209884644)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-61a32a226461>:65: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  embedding_vector = new_model[word.upper()]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "MAX_SEQUENCE_LENGTH = 450\n",
    "MAX_NB_WORDS = len(new_model.wv.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=MAX_NB_WORDS, lower=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "print(MAX_NB_WORDS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "\n",
    "\n",
    "print (len(sequences))\n",
    "#print (sequences[0])\n",
    "print (type(word_index))\n",
    "#print (word_index)\n",
    "\n",
    "\n",
    "# ensure that all sequences have the same length using pad\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\"\"\"tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=None, dtype='int32', padding='pre', truncating='post',\n",
    "    value=0.0)\"\"\"\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "\n",
    "print('Shape of data tensor:',data.shape)\n",
    "\n",
    "\n",
    "labels = np.vstack((np.ones((753, 1)),np.zeros((753,1))))\n",
    "\n",
    "#labels_one_dim = labels.reshape(labels.shape[0], )\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "print (embedding_matrix.shape)\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    if word.upper() in new_model.wv.vocab:\n",
    "        embedding_vector = new_model[word.upper()]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix[10])\n",
    "print(len(embedding_matrix))\n",
    "print(new_model.wv.most_similar('LDI'))\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Embedding, LSTM, GRU\n",
    "from keras.initializers import Constant\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import Input, Dense, Lambda, LSTM, RepeatVector,Dropout, Conv1D, MaxPooling1D, UpSampling1D,GlobalMaxPool1D\n",
    "\n",
    "\n",
    "num_words_plus=len(word_index) + 1\n",
    "model=Sequential()\n",
    "embedding_layer = Embedding(num_words_plus,\n",
    "                        EMBEDDING_DIM,\n",
    "                        #embeddings_initializer=Constant(embedding_matrix),\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        trainable=False)\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "def rnn_architecture():\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), )\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    # before recurrent_dropout was 0.5\n",
    "    x = Bidirectional(LSTM(32, dropout=0.5, recurrent_dropout=0.1, return_sequences = True))(embedded_sequences)\n",
    "    x = Bidirectional(LSTM(32, dropout=0.5,recurrent_dropout=0.1))(x)\n",
    "    preds = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer= 'adam',\n",
    "                  metrics=['accuracy']\n",
    "             )\n",
    "    return model\n",
    "\n",
    "model=rnn_architecture()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 [==============================] - 120s 3s/step - loss: 0.6076 - accuracy: 0.6703 - val_loss: 0.3881 - val_accuracy: 0.8400\n",
      "Epoch 2/50\n",
      "37/40 [==========================>...] - ETA: 9s - loss: 0.3869 - accuracy: 0.8336 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2218cd93599f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_te\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SG_old_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=rnn_architecture()\n",
    "\n",
    "n_seqs=len(texts)\n",
    "\n",
    "indices = np.arange(n_seqs)\n",
    "np.random.shuffle(indices)\n",
    "X = data[indices]\n",
    "y = labels[indices]\n",
    "\n",
    "n_tr = int(n_seqs * 0.85)\n",
    "n_va = int(n_seqs * 0.05)\n",
    "n_te = n_seqs - n_tr - n_va\n",
    "X_train = X[:n_tr]\n",
    "y_train = y[:n_tr]\n",
    "X_valid = X[n_tr:n_tr+n_va]\n",
    "y_valid = y[n_tr:n_tr+n_va]\n",
    "X_test = X[-n_te:]\n",
    "y_test = y[-n_te:]\n",
    "\n",
    "model.fit(X_train,y_train,epochs=50, batch_size=32,shuffle=True, validation_data=(X_valid,y_valid), verbose=1)\n",
    "model.save('SG_old_model.h5')\n",
    "\n",
    "\"\"\"\n",
    "rneg_test=r\"Neg_testing.fasta\"\n",
    "Pos_test=r\"AMRProt_testing_Positive\"\n",
    "\n",
    "test_texts_P=[]\n",
    "for index, record in enumerate(SeqIO.parse(Pos_test,\"fasta\")):\n",
    "    tri_tokens = trigrams(record.seq)\n",
    "    temp_str = \"\"\n",
    "    for item in ((tri_tokens)):\n",
    "        #print(item),\n",
    "        temp_str = temp_str + \" \" +item[0] + item[1] + item[2]\n",
    "    #print (temp_str)\n",
    "    test_texts_P.append(temp_str)\n",
    "print(len(test_texts_P))\n",
    "\n",
    "P_test=[p_test for p_test in test_texts_P if p_test not in texts]\n",
    "\n",
    "P_test=P_test[0:305]\n",
    "\n",
    "\n",
    "test_texts_N=[]\n",
    "for index, record in enumerate(SeqIO.parse(rneg_test,\"fasta\")):\n",
    "    tri_tokens = trigrams(record.seq)\n",
    "    temp_str = \"\"\n",
    "    for item in ((tri_tokens)):\n",
    "        #print(item),\n",
    "        temp_str = temp_str + \" \" +item[0] + item[1] + item[2]\n",
    "    #print (temp_str)\n",
    "    test_texts_N.append(temp_str)\n",
    "\n",
    "\n",
    "\n",
    "N_test=[n_test for n_test in test_texts_N if n_test not in texts]\n",
    "N_test=N_test[0:305]\n",
    "\n",
    "test_texts=P_test + N_test\n",
    "\n",
    "with open('tokenizer_NewPat.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(test_texts)\n",
    "sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "\n",
    "#word_index = tokenizer.word_index\n",
    "#print(word_index)\n",
    "data_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding=\"post\")\n",
    "\n",
    "\n",
    "train_results=model.evaluate(X_test,y_test,verbose=1)\n",
    "\n",
    "\n",
    "labels = np.vstack((np.ones((305,1)),\n",
    "                    np.zeros((305,1))))\n",
    "\n",
    "y_test=labels \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred=model.predict(X_test)\n",
    "y_pred=np.where(pred > 0.5,1,0)\n",
    "print(len(y_pred))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(recall_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from numpy import concatenate\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import model_selection\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.semi_supervised import LabelPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.50, random_state=1, stratify=y)\n",
    "# split train into labeled and unlabeled\n",
    "X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, test_size=0.50, random_state=1, stratify=y_train)\n",
    "# create the training dataset input\n",
    "X_train_mixed = concatenate((X_train_lab, X_test_unlab))\n",
    "# create \"no label\" for unlabeled data\n",
    "nolabel = [-1 for _ in range(len(y_test_unlab))]\n",
    "# recombine training dataset labels\n",
    "y_train_mixed = concatenate((y_train_lab, nolabel))\n",
    "# define model\n",
    "model = LabelPropagation()\n",
    "#len(nolabel)\n",
    "y_train_mixed\n",
    "#X_train_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on training dataset\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "\n",
    "model.fit(X_train_mixed, y_train_mixed)\n",
    "# make predictions on hold out test set\n",
    "yhat = model.predict(X_test)\n",
    "# calculate score for test set\n",
    "score = accuracy_score(y_test, yhat)\n",
    "# summarize score\n",
    "print('Accuracy: %.3f' % (score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d0f39d75a017>:2: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(file_name,\"rU\") as handle:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SeqIO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d0f39d75a017>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;31m#print(curated_seq_positives[0:5])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#chars_per_line(r\"C:\\Users\\AMD\\Desktop\\Specific\\metals\\BacMet2_EXP_database_1\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mchars_per_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\AMD\\Desktop\\Pre_metall\\50%_Pre_BacMet.1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-d0f39d75a017>\u001b[0m in \u001b[0;36mchars_per_line\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[1;31m#outputfile=open(\"The positive cases\",\"w\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;31m#nega_bact=open(\"Negative set\",'w')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0mPosit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSeqIO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fasta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mPosit_lis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrecord\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPosit\u001b[0m                          \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SeqIO' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6\n",
      "1 8\n",
      "   0  1    2\n",
      "0  2  3  boy\n",
      "1  4  6  boy\n"
     ]
    }
   ],
   "source": [
    "x=[[2,3,6],[4,6,8]]\n",
    "x=pd.DataFrame(x)\n",
    "\n",
    "for i,j in enumerate(x[2]):\n",
    "    print(i,j)\n",
    "    if j >5:\n",
    "        x[2]=x[2].replace(j,\"boy\")\n",
    "        #x.at[i,x[2]]=y\n",
    "print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>boy</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2    6    8\n",
       "0  2  3  6  boy  boy\n",
       "1  4  6  8  NaN  NaN"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
